---
title: "Biostat 203B Homework 5 / Logistic regression with enet"
subtitle: Due Mar 20 @ 11:59PM
author: "Ningke Zhang 705834790"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
---

1. Load libraries
```{r}
library(tidymodels)
library(dplyr)
library(recipes)
library(workflows)
library(tune)
library(glmnet)
library(vip)
library(ranger)
library(future)
library(xgboost)
```
2.Data preprocessing and feature engineering.
```{r}
# read data
mimiciv_icu_cohort <- readRDS("/Users/ningkezhang/Downloads/203b-hw-revised/hw4/mimiciv_shiny/mimic_icu_cohort.rds") |> #as Bowen mentioned, no need to copy rds in hw5, so this line have to exceed 80.
  select(-c(intime, 
            outtime, 
            admittime,
            dischtime,
            deathtime,
            admit_provider_id,
            edregtime,
            edouttime,
            anchor_age,
            anchor_year,
            anchor_year_group,
            last_careunit,
            discharge_location,
            hospital_expire_flag,
            dod,
            los)
         ) |>
  mutate(los_long = as.factor(los_long)) |>
  print(width = Inf)
```
3.Data split
```{r}
set.seed(203)

mimiciv_icu_cohort <- mimiciv_icu_cohort |>
  arrange(subject_id, hadm_id, stay_id) |>
  select(-c(subject_id, hadm_id, stay_id))
mimiciv_icu_cohort <- mimiciv_icu_cohort |> drop_na()

data_split <- initial_split(mimiciv_icu_cohort, 
                            strata = "los_long", 
                            prop = 0.5)

icu_other <- training(data_split)
icu_test <- testing(data_split)
```
4.Train logistic regression with elasticnet regularization.
```{r}
# Define the recipe
gb_recipe <- 
  recipe(los_long ~ ., data = icu_other) |>
  step_impute_median(all_numeric_predictors()) |>
  step_impute_mode(all_nominal_predictors()) |>
  step_unknown(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_nzv(all_predictors()) |>  
  step_normalize(all_numeric_predictors(), -all_outcomes())

# Define the model
gb_mod <-
  boost_tree(
    mode = "classification",
    trees = 600, 
    tree_depth = tune(),
    learn_rate = tune()
  ) |> 
  set_engine("xgboost")
gb_mod

# Define the workflow
gb_wf <- workflow() |>
  add_recipe(gb_recipe) |>
  add_model(gb_mod)
gb_wf

# Define the grid
param_grid <- grid_regular(
  tree_depth(range = c(3L, 8L)),  
  learn_rate(range = c(-3, -0.5), trans = log10_trans()),
  levels = c(5, 5)
)
```
5.Cross-validation
```{r}
set.seed(203)

folds <- vfold_cv(icu_other, v = 5, strata = los_long)

# fit cross-validation
gb_fit <- gb_wf |>
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy),
    control = control_grid(verbose = TRUE, save_pred = TRUE)
  )
gb_fit

#visualize CV results
gb_fit |>
  collect_metrics() |> 
  filter(.metric == "roc_auc") |>
  ggplot(aes(x = learn_rate, y = mean, color = factor(tree_depth), 
             group = factor(tree_depth))) +
  geom_point(size = 3, alpha = 0.7) + 
  geom_line(linewidth = 1) +
  labs(
    title = "Gradient Boosting: Learning Rate vs AUC",
    x = "Learning Rate",
    y = "Cross-Validation AUC",
    color = "Tree Depth"
  ) +
  scale_x_log10() +
  theme_minimal()
```
6. Model evaluation
```{r}
# select the best model
best_gb <- gb_fit |> select_best(metric = "roc_auc")
print(best_gb)

# finalize the workflow/fit
final_gb_wf <- finalize_workflow(gb_wf, best_gb)

final_gb_fit <- final_gb_wf |> last_fit(data_split)

saveRDS(final_gb_fit, "final_fit_gb_lastfit.rds")

final_gb_model <- final_gb_fit |> extract_workflow() |> extract_fit_parsnip()

final_gb_model |> vip()

saveRDS(final_gb_model, "final_fit_gb.rds")
```